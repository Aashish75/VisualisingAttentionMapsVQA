<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Visualising Attention Maps</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="navbar">
        <a href="index.html" class="active">Home</a>
        <a href="tryit.html">Try it Yourself</a>
        <a href="citations.html">Citations</a>
    </div>
    <div class="content">
        <aside class="sidebar">
            <a href="#introduction" class="side-link">Introduction</a>
            <a href="#method1" class="side-link">Method 1</a>
            <a href="#method2" class="side-link">Method 2</a>
            <a href="#method3" class="side-link">Method 3</a>
        </aside>
        <main class="container">
            <h1>Visualising Attention Maps as Evidence for Visual Question Answering</h1>
            <h2 id="introduction">Introduction</h2>
            <hr>
            <p>The question of how one should choose among competing explanations (models) of observed data is at the core of science. Model comparison is ubiquitous and arises, for example, when a toxicologist must decide between two dose–response models or when a biochemist needs to determine which of a set of enzyme-kinetics models best accounts for observed data.
                Over the decades, a number of criteria that are thought to be important for model comparison have been proposed (e.g., Jacobs and Grainger1). They include (1) falsifiability2: whether there exist potential observations that are incompatible with the model; (2) explanatory adequacy: whether the theoretical account of the model helps to make sense of observed data but also established findings; (3) interpretability: whether the components of the model, especially its parameters, are understandable and are linked to known processes; (4) faithfulness: whether the ability of the model to capture the underlying regularities comes from the theoretical principles the model purports to implement, not from the incidental choices made in its computational instantiation; (5) goodness of fit: whether the model fits the observed data sufficiently well; (6) complexity or simplicity: whether the model's description of observed data is achieved in the simplest possible manner; and (7) generalizability: whether the model provides a good prediction of future observations.
                Although each of these seven criteria is important in its own way, modern statistical approaches to model comparison consider only the last of these three (goodness of fit, complexity, and generalizability), largely because they lend themselves to quantification. The other four criteria have yet to be formalized and it is not clear how some even could be or should be (e.g., interpretability).</p>

            <h3 id="method1">Method 1</h3>
            <hr>
            <p>&lt;The question of how one should choose among competing explanations (models) of observed data is at the core of science. Model comparison is ubiquitous and arises, for example, when a toxicologist must decide between two dose–response models or when a biochemist needs to determine which of a set of enzyme-kinetics models best accounts for observed data.
                Over the decades, a number of criteria that are thought to be important for model comparison have been proposed (e.g., Jacobs and Grainger1). They include (1) falsifiability2: whether there exist potential observations that are incompatible with the model; (2) explanatory adequacy: whether the theoretical account of the model helps to make sense of observed data but also established findings; (3) interpretability: whether the components of the model, especially its parameters, are understandable and are linked to known processes; (4) faithfulness: whether the ability of the model to capture the underlying regularities comes from the theoretical principles the model purports to implement, not from the incidental choices made in its computational instantiation; (5) goodness of fit: whether the model fits the observed data sufficiently well; (6) complexity or simplicity: whether the model's description of observed data is achieved in the simplest possible manner; and (7) generalizability: whether the model provides a good prediction of future observations.
                Although each of these seven criteria is important in its own way, modern statistical approaches to model comparison consider only the last of these three (goodness of fit, complexity, and generalizability), largely because they lend themselves to quantification. The other four criteria have yet to be formalized and it is not clear how some even could be or should be (e.g., interpretability).&gt;<br>&lt;image of Attention map&gt;</p>

            <h3 id="method2">Method 2</h3>
            <hr>
            <p>&lt;The question of how one should choose among competing explanations (models) of observed data is at the core of science. Model comparison is ubiquitous and arises, for example, when a toxicologist must decide between two dose–response models or when a biochemist needs to determine which of a set of enzyme-kinetics models best accounts for observed data.
                Over the decades, a number of criteria that are thought to be important for model comparison have been proposed (e.g., Jacobs and Grainger1). They include (1) falsifiability2: whether there exist potential observations that are incompatible with the model; (2) explanatory adequacy: whether the theoretical account of the model helps to make sense of observed data but also established findings; (3) interpretability: whether the components of the model, especially its parameters, are understandable and are linked to known processes; (4) faithfulness: whether the ability of the model to capture the underlying regularities comes from the theoretical principles the model purports to implement, not from the incidental choices made in its computational instantiation; (5) goodness of fit: whether the model fits the observed data sufficiently well; (6) complexity or simplicity: whether the model's description of observed data is achieved in the simplest possible manner; and (7) generalizability: whether the model provides a good prediction of future observations.
                Although each of these seven criteria is important in its own way, modern statistical approaches to model comparison consider only the last of these three (goodness of fit, complexity, and generalizability), largely because they lend themselves to quantification. The other four criteria have yet to be formalized and it is not clear how some even could be or should be (e.g., interpretability).&gt;<br>&lt;image of Attention map&gt;</p>

            <h3 id="method3">Method 3</h3>
            <hr>
            <p>&lt;The question of how one should choose among competing explanations (models) of observed data is at the core of science. Model comparison is ubiquitous and arises, for example, when a toxicologist must decide between two dose–response models or when a biochemist needs to determine which of a set of enzyme-kinetics models best accounts for observed data.
                Over the decades, a number of criteria that are thought to be important for model comparison have been proposed (e.g., Jacobs and Grainger1). They include (1) falsifiability2: whether there exist potential observations that are incompatible with the model; (2) explanatory adequacy: whether the theoretical account of the model helps to make sense of observed data but also established findings; (3) interpretability: whether the components of the model, especially its parameters, are understandable and are linked to known processes; (4) faithfulness: whether the ability of the model to capture the underlying regularities comes from the theoretical principles the model purports to implement, not from the incidental choices made in its computational instantiation; (5) goodness of fit: whether the model fits the observed data sufficiently well; (6) complexity or simplicity: whether the model's description of observed data is achieved in the simplest possible manner; and (7) generalizability: whether the model provides a good prediction of future observations.
                Although each of these seven criteria is important in its own way, modern statistical approaches to model comparison consider only the last of these three (goodness of fit, complexity, and generalizability), largely because they lend themselves to quantification. The other four criteria have yet to be formalized and it is not clear how some even could be or should be (e.g., interpretability).&gt;<br>&lt;image of Attention map&gt;</p>
        </main>
    </div>
    <script src="script.js"></script>
</body>
</html>
